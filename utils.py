import torch
import torch.nn as nn
from typing import List, Tuple, Dict
from metrics import compute_all_metrics
import matplotlib.pyplot as plt
import numpy as np
from loss import ComboLoss
import torch.optim as optim
from stats import plot_metrics_evolution


def train_one_epoch(model: nn.Module, 
                    dataloader: torch.utils.data.DataLoader, 
                    optimizer: torch.optim.Optimizer, 
                    loss_fn: nn.Module, 
                    device: torch.device, 
                    num_classes: int = 4) -> float:
    """Entrena el modelo por una √©poca.
    
    Args:
        model: Modelo a entrenar
        dataloader: DataLoader con datos de entrenamiento
        optimizer: Optimizador
        loss_fn: Funci√≥n de p√©rdida
        device: Dispositivo (CPU/GPU)
        num_classes: N√∫mero de clases
        
    Returns:
        P√©rdida promedio en la √©poca
    """
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    for batch in dataloader:
        # Manejar diferentes formatos de batch (im√°genes, m√°scaras, posiblemente otros datos)
        images = batch[0].to(device)
        masks = batch[1]
        
        # Preprocesar m√°scaras
        if masks.dim() == 4 and masks.size(1) == 1:
            masks = masks.squeeze(1)
        masks = masks.long().to(device)

        # Validar rango de las m√°scaras
        if masks.min() < 0 or masks.max() >= num_classes:
            raise ValueError(f"Las etiquetas est√°n fuera de rango: {masks.min()} a {masks.max()}")
        
        # Paso forward
        outputs = model(images)
        loss = loss_fn(outputs, masks)

        # Paso backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1

    return total_loss / max(num_batches, 1)


def evaluate(model: nn.Module, 
             dataloader: torch.utils.data.DataLoader, 
             loss_fn: nn.Module, 
             device: torch.device, 
             num_classes: int = 4) -> Tuple[float, Dict[str, float]]:
    """Eval√∫a el modelo en el conjunto de validaci√≥n con todas las m√©tricas.
    
    Args:
        model: Modelo a evaluar
        dataloader: DataLoader con datos de validaci√≥n
        loss_fn: Funci√≥n de p√©rdida
        device: Dispositivo (CPU/GPU)
        num_classes: N√∫mero de clases
        
    Returns:
        Tuple con (p√©rdida promedio, diccionario con todas las m√©tricas)
    """
    model.eval()
    total_loss = 0.0
    all_metrics = {}
    num_batches = 0
    
    with torch.no_grad():
        for batch in dataloader:
            images = batch[0].to(device)
            masks = batch[1]
            
            # Preprocesar m√°scaras
            if masks.dim() == 4 and masks.size(1) == 1:
                masks = masks.squeeze(1)
            masks = masks.long().to(device)

            # Validar rango de las m√°scaras
            if masks.min() < 0 or masks.max() >= num_classes:
                raise ValueError(f"Las etiquetas est√°n fuera de rango: {masks.min()} a {masks.max()}")
            
            # Forward pass
            outputs = model(images)
            
            # Calcular p√©rdida
            total_loss += loss_fn(outputs, masks).item()
            
            # Calcular todas las m√©tricas
            batch_metrics = compute_all_metrics(outputs, masks, num_classes)
            
            # Acumular m√©tricas
            for key, value in batch_metrics.items():
                if key not in all_metrics:
                    all_metrics[key] = []
                all_metrics[key].append(value)
            
            num_batches += 1
    
    # Promediar m√©tricas
    avg_metrics = {}
    for key, values in all_metrics.items():
        # Filtrar valores infinitos para HD
        if 'hd95' in key:
            finite_values = [v for v in values if v != float('inf')]
            avg_metrics[key] = np.mean(finite_values) if finite_values else float('inf')
        else:
            avg_metrics[key] = np.mean(values)
    
    avg_loss = total_loss / max(num_batches, 1)
    
    return avg_loss, avg_metrics


def show_predictions(model: nn.Module, 
                    dataloader: torch.utils.data.DataLoader, 
                    device: torch.device, 
                    num_classes: int = 4,
                    num_examples: int = 3):
    """Muestra ejemplos de predicciones del modelo.
    
    Args:
        model: Modelo entrenado
        dataloader: DataLoader con datos
        device: Dispositivo (CPU/GPU)
        num_classes: N√∫mero de clases
        num_examples: N√∫mero de ejemplos a mostrar
    """
    model.eval()
    
    try:
        # Obtener un batch de datos
        batch = next(iter(dataloader))
        images = batch[0].to(device)
        masks = batch[1]
        
        # Preprocesar m√°scaras
        if masks.dim() == 4 and masks.size(1) == 1:
            masks = masks.squeeze(1)
        masks = masks.to(device)

        # Obtener predicciones
        with torch.no_grad():
            preds = model(images)
            preds = torch.argmax(preds, dim=1)  # Convertir a clases predichas

        # Mover datos a CPU para visualizaci√≥n
        images = images.cpu()
        masks = masks.cpu()
        preds = preds.cpu()

        # Mostrar ejemplos
        for i in range(min(num_examples, images.size(0))):
            fig, axs = plt.subplots(1, 3, figsize=(15, 5))

            # Imagen original
            axs[0].imshow(images[i].permute(1, 2, 0))
            axs[0].set_title("Imagen original")
            axs[0].axis("off")

            # M√°scara verdadera
            axs[1].imshow(masks[i], cmap="tab10", vmin=0, vmax=num_classes-1)
            axs[1].set_title("M√°scara verdadera")
            axs[1].axis("off")

            # Predicci√≥n
            axs[2].imshow(preds[i], cmap="tab10", vmin=0, vmax=num_classes-1)
            axs[2].set_title("Predicci√≥n")
            axs[2].axis("off")

            plt.tight_layout()
            plt.show()
            
    except Exception as e:
        print(f"Error al visualizar predicciones: {str(e)}")
def train_model(model: nn.Module, 
                train_loader: torch.utils.data.DataLoader, 
                test_loader: torch.utils.data.DataLoader, 
                epochs: int = 10, 
                lr: float = 1e-4,
                num_classes: int = 4) -> Tuple[nn.Module, List[float], List[float], List[Dict[str, float]]]:
    """Entrena y eval√∫a el modelo con m√©tricas completas.
    
    Args:
        model: Modelo a entrenar
        train_loader: DataLoader de entrenamiento
        test_loader: DataLoader de validaci√≥n
        epochs: N√∫mero de √©pocas
        lr: Tasa de aprendizaje
        num_classes: N√∫mero de clases
        
    Returns:
        Tuple con (modelo entrenado, p√©rdidas de entrenamiento, p√©rdidas de validaci√≥n, m√©tricas de validaci√≥n)
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # Inicializar funci√≥n de p√©rdida y optimizador
    loss_fn = ComboLoss(alpha=0.5, beta=0.5)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # Historial de m√©tricas
    train_losses = []
    val_losses = []
    val_metrics_history = []

    print("üöÄ Iniciando entrenamiento con m√©tricas completas...")
    print("üìä M√©tricas incluidas: Dice, Accuracy, Precision, Recall, IoU, HD95")
    print("-" * 80)

    for epoch in range(epochs):
        print(f"üöÄ √âpoca {epoch+1}/{epochs}")
        
        # Fase de entrenamiento
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, num_classes)
        train_losses.append(train_loss)
        
        # Fase de evaluaci√≥n
        val_loss, val_metrics = evaluate(model, test_loader, loss_fn, device, num_classes)
        val_losses.append(val_loss)
        val_metrics_history.append(val_metrics)
        
        # Mostrar m√©tricas principales
        print(f"üìä Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        print(f"üìä Dice: {val_metrics['dice_avg']:.4f} | Accuracy: {val_metrics['pixel_accuracy']:.4f}")
        print(f"üìä Precision: {val_metrics['precision_avg']:.4f} | Recall: {val_metrics['recall_avg']:.4f}")
        print(f"üìä IoU: {val_metrics['iou_avg']:.4f} | HD95: {val_metrics['hd95_avg']:.2f}")
        print("-" * 80)
        
    # Visualizar resultados finales
    print("üéØ Mostrando predicciones finales...")
    show_predictions(model, test_loader, device, num_classes)
    
    # Graficar evoluci√≥n de m√©tricas
    print("üìà Graficando evoluci√≥n de m√©tricas...")
    plot_metrics_evolution(val_metrics_history, num_classes)
    
    # Mostrar resumen final
    final_metrics = val_metrics_history[-1]
    print("\nüèÜ RESUMEN FINAL DE M√âTRICAS:")
    print(f"  ‚Ä¢ Dice Score: {final_metrics['dice_avg']:.4f}")
    print(f"  ‚Ä¢ Pixel Accuracy: {final_metrics['pixel_accuracy']:.4f}")
    print(f"  ‚Ä¢ Precision: {final_metrics['precision_avg']:.4f}")
    print(f"  ‚Ä¢ Recall: {final_metrics['recall_avg']:.4f}")
    print(f"  ‚Ä¢ IoU: {final_metrics['iou_avg']:.4f}")
    print(f"  ‚Ä¢ HD95: {final_metrics['hd95_avg']:.2f} p√≠xeles")
    
    print("\nüìã M√âTRICAS POR CLASE:")
    for cls in range(num_classes):
        print(f"  Clase {cls}:")
        print(f"    - Dice: {final_metrics[f'dice_class_{cls}']:.4f}")
        print(f"    - Precision: {final_metrics[f'precision_class_{cls}']:.4f}")
        print(f"    - Recall: {final_metrics[f'recall_class_{cls}']:.4f}")
        print(f"    - IoU: {final_metrics[f'iou_class_{cls}']:.4f}")
        hd_val = final_metrics[f'hd95_class_{cls}']
        hd_str = f"{hd_val:.2f}" if hd_val != float('inf') else "‚àû"
        print(f"    - HD95: {hd_str} p√≠xeles")
    
    return model, train_losses, val_losses, val_metrics_history